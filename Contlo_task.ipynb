{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "metadata": {
        "id": "AhA_AR1bjX68"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration class for GPT model hyperparameters"
      ],
      "metadata": {
        "id": "twlD0iVU21XF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTConfig:\n",
        "    attn_dropout = 0.1\n",
        "    embed_dropout = 0.1\n",
        "    ff_dropout = 0.1\n",
        "\n",
        "    def __init__(self, vocab_size, max_len, **kwargs):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_len = max_len\n",
        "        # Additional hyperparameters can be set dynamically\n",
        "        for key, value in kwargs.items():\n",
        "            setattr(self, key, value)\n",
        "\n",
        "# GPT1Config extends GPTConfig with specific hyperparameters\n",
        "class GPT1Config(GPTConfig):\n",
        "    num_heads = 12\n",
        "    num_blocks = 12\n",
        "    embed_dim = 768"
      ],
      "metadata": {
        "id": "M1a029GajljP"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.encoding = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.encoding = self.encoding.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.encoding[:, :x.size(1)].detach()\n"
      ],
      "metadata": {
        "id": "LqA_Bhxsrmv_"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main GPT model class"
      ],
      "metadata": {
        "id": "QQO8Qich3Op8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # Define layers of the GPT model using ModuleDict\n",
        "        self.layers = nn.ModuleDict({\n",
        "            'tok_embed': nn.Embedding(config.vocab_size, config.embed_dim),\n",
        "            'pos_embed': PositionalEncoding(config.embed_dim, config.max_len),\n",
        "            'dropout': nn.Dropout(config.embed_dropout),\n",
        "            'blocks': nn.Sequential(*[Block(config) for _ in range(config.num_blocks)]),\n",
        "            'ln': nn.LayerNorm(config.embed_dim),\n",
        "            'fc': nn.Linear(config.embed_dim, config.vocab_size)\n",
        "        })\n",
        "        # Initialize weights using Xavier uniform initialization\n",
        "        self.apply(self.weights_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "      # Forward pass through the GPT model\n",
        "        tok_embedding = self.layers['tok_embed'](x)\n",
        "        pos_embedding = self.layers['pos_embed'](tok_embedding)\n",
        "        x = self.layers['dropout'](tok_embedding + pos_embedding)\n",
        "        x = self.layers['blocks'](x)\n",
        "        x = self.layers['ln'](x)\n",
        "        x = self.layers['fc'](x)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def weights_init(module):\n",
        "      # Initialize weights of Linear and Embedding layers using Xavier uniform initialization\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            nn.init.xavier_uniform_(module.weight.data)\n",
        "             # Initialize bias to zero if present\n",
        "            if hasattr(module, 'bias') and module.bias is not None:\n",
        "                nn.init.constant_(module.bias.data, 0)\n"
      ],
      "metadata": {
        "id": "PWRdEpW-juvC"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Transformer block class"
      ],
      "metadata": {
        "id": "ErHmUUse3okT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "vjZ5bn9ijUlz"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        embed_dim = config.embed_dim\n",
        "        # Define layers of the transformer block using ModuleDict\n",
        "\n",
        "        self.layers = nn.ModuleDict({\n",
        "            'ln1': nn.LayerNorm(embed_dim),\n",
        "            'ln2': nn.LayerNorm(embed_dim),\n",
        "            'attn': MultiheadAttention(config),\n",
        "            'ff': nn.Sequential(\n",
        "                nn.Linear(embed_dim, embed_dim * 4),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(embed_dim * 4, embed_dim),\n",
        "                nn.Dropout(config.ff_dropout),\n",
        "            )\n",
        "        })\n",
        "\n",
        "    def forward(self, x):\n",
        "       # Forward pass through the transformer block\n",
        "        x = x + self.layers['attn'](self.layers['ln1'](x))\n",
        "        x = x + self.layers['ff'](self.layers['ln2'](x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multihead Attention class"
      ],
      "metadata": {
        "id": "tHWvfTlS31lK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        embed_dim = config.embed_dim\n",
        "        self.num_heads = config.num_heads\n",
        "        assert embed_dim % self.num_heads == 0, \"invalid heads and embedding dimension configuration\"\n",
        "        # Define layers of the multihead attention mechanism using ModuleDict\n",
        "        self.layers = nn.ModuleDict({\n",
        "            'key': nn.Linear(embed_dim, embed_dim),\n",
        "            'value': nn.Linear(embed_dim, embed_dim),\n",
        "            'query': nn.Linear(embed_dim, embed_dim),\n",
        "            'proj': nn.Linear(embed_dim, embed_dim),\n",
        "            'attn_dropout': nn.Dropout(config.attn_dropout),\n",
        "            'proj_dropout': nn.Dropout(config.ff_dropout),\n",
        "        })\n",
        "\n",
        "        # Create a triangular mask to prevent attending to future tokens\n",
        "\n",
        "        self.register_buffer(\n",
        "    \"mask\",\n",
        "    torch.tril(torch.ones(config.max_len, config.max_len), diagonal=-1).unsqueeze(0).unsqueeze(0).bool()\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      # Forward pass through the multihead attention mechanism\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "        seq_len = x.size(1)\n",
        "        k_t = self.layers['key'](x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        v = self.layers['value'](x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        q = self.layers['query'](x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "\n",
        "        attn = torch.matmul(q, k_t) / math.sqrt(q.size(-1))\n",
        "        mask = self.mask[:, :, :seq_len, :seq_len]\n",
        "        attn = attn.masked_fill(mask == 0, float(\"-inf\"))\n",
        "        attn = self.layers['attn_dropout'](attn)\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        y = torch.matmul(attn, v)\n",
        "        y = y.transpose(1, 2)\n",
        "        y = y.reshape(batch_size, seq_len, -1)\n",
        "        y = self.layers['proj_dropout'](self.layers['proj'](y))\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "M6nXzOhjlWPp"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage with 125 million parameters\n",
        "gpt_config = GPT1Config(vocab_size=10000, max_len=512, embed_dim=768, num_heads=12, num_blocks=12)\n",
        "gpt_model = GPT(gpt_config)\n",
        "input_sequence = torch.randint(0, gpt_config.vocab_size, (1, gpt_config.max_len))\n",
        "output = gpt_model(input_sequence)\n",
        "print(f\"Output shape: {output.shape}, Model Parameters: {sum(p.numel() for p in gpt_model.parameters()):,} parameters\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-OCXi6G1w9P",
        "outputId": "fcb09495-effc-4fad-a4c0-6e82f820a1a2"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([1, 512, 10000]), Model Parameters: 100,426,000 parameters\n"
          ]
        }
      ]
    }
  ]
}